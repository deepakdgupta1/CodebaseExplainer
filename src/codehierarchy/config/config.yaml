system:
  max_memory_gb: 26.0
  temp_dir: ".codehierarchy/temp"
  output_dir: "output"
  checkpointing_enabled: true
  checkpoint_interval: 100

parsing:
  languages:
    - "python"
    - "typescript"
  num_workers: 6
  timeout_seconds: 60
  max_file_size_mb: 10.0
  exclude_patterns:
    - "**/node_modules/**"
    - "**/venv/**"
    - "**/__pycache__/**"
    - "**/.git/**"
    - "**/dist/**"
    - "**/build/**"
    - "**/*.min.js"
    - "**/*.test.ts"
    - "**/tests/**"

graph:
  storage_mode: "in_memory"
  cache_size_gb: 2.0
  compute_metrics: true

llm:
  backend: "llamacpp"  # "llamacpp" or "lmstudio"
  model_name: "Qwen2.5-Coder-32B-Instruct.IQ4_XS.gguf"
  base_url: "http://localhost:8080/v1"
  api_key: "not-needed"
  context_window: 32768
  batch_size: 10
  temperature: 0.2
  timeout_seconds: 300
  max_retries: 2
  # Advanced Settings
  context_overflow_policy: "stopAtLimit"
  top_k: 40
  repeat_penalty: 1.1
  min_p: 0.05
  top_p: 0.95
  gpu_offload_ratio: 0.0
  cpu_threads: 4
  eval_batch_size: 8
  flash_attention: false
  use_mmap: true

  # LM Studio specific
  lmstudio:
    appimage_path: "/home/deeog/Desktop/LMStudio.AppImage"  # Auto-detect
    use_xvfb: true
    port: 1234

  # llama.cpp specific
  llamacpp:
    server_path: "/home/deeog/Desktop/llama.cpp/build/bin/llama-server"
    model_path: "/home/deeog/.lmstudio/models/custom_user/qwen2.5-coder-32b-custom/Qwen2.5-Coder-32B-Instruct.IQ4_XS.gguf"
    port: 8080
    n_gpu_layers: -1

embeddings:
  model_name: "all-mpnet-base-v2"
  dimension: 768
  batch_size: 32

search:
  default_mode: "hybrid"
  index_type: "IVF256,Flat"
  nprobe: 64
  rrf_k: 60
